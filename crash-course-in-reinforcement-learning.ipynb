{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d0726d4ef457df4a28173b37445ad500e90459a8"
   },
   "source": [
    "![openAI](https://openai.com/assets/images/home/openai-homepage@2x-4e2e39cbd1.svg)\n",
    "Founded with the aim to build a safe artificial general intelligence, OpenAI have developed the Gym platform. This is a collection of environments and problems designed to flex the muscles of reinforcement learning algorithms. \n",
    "\n",
    "Let's get our gym membership..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a266b510f100f8f91ba407da581a6298facff597"
   },
   "source": [
    "You're just off the phone to  Uber, they're having a bit of trouble with their new self driving car and they want you to fix it.\n",
    "\n",
    "\"No worries, Dara Khosrowshahi, I've got this under control!\", *click*.\n",
    "\n",
    "There are **`4`** locations represented by different letters and it's your job to pick up the passenger at one location and drop them off in another.\n",
    "\n",
    "Let's load up our first environment `Taxi-v2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "84d59004d5dbd9023868fe7ec29529177bb7416f"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6e528e76fa7ccdb7a3fb74ad68e5a1d3c3e62d97"
   },
   "source": [
    "We can view the state of our environment by using `render`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "ea998fbfd4f0ffcb4436461df3efd219ff255477",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e190c9a9aab248ac4fc62967d64b344eef63ff8d"
   },
   "source": [
    "Now we can see our environment!\n",
    "* Yellow square represents the taxi\n",
    "* “|” represents a wall\n",
    "* The letter coloured blue represents the passenger pick-up location\n",
    "* The letter coloured purple represents the passengers destination \n",
    "\n",
    "**Note** The taxi will turn green when a passenger has been picked up.\n",
    "\n",
    "There are six possible moves a taxi can take:\n",
    "* Up `0`\n",
    "* Down `1`\n",
    "* Right `2`\n",
    "* Left `3`\n",
    "* Pickup `4`\n",
    "* Dropoff `5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc5236bdbdeda8999ab924b0daff78894bd9c05f"
   },
   "source": [
    "Let's see a movement in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "bfc7e8931a7959a8472e4d15583a103c092b29f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random state and render it\n",
    "env.env.s = 420\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "872b8892b9c0e63205b5c0f4f63a364a3ab12e23"
   },
   "source": [
    "Let's move to the right by taking a `step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "4d4e5c033f406acd1403471cdafdf4b9f3c2cd28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (East)\n"
     ]
    }
   ],
   "source": [
    "# Remember right corresponds to 2\n",
    "env.step(2)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "17c2c5d12124f99d45295f0a9da7ec6180e20190"
   },
   "source": [
    "See, we've moved a space to the right. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "301a9858ec5738624a74d173332be060ef3d2aad"
   },
   "source": [
    "Let's look a bit more closely at the output of `step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "fcac0d5aa562e3fb8e927629062a339ed98a65c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440, -1, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fcc6613ce1a818fb64da92bcb6eb6c525710359f"
   },
   "source": [
    "These should be unpacked like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "0d38f4bc063eb7ec772fef740121c59405357662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: 440\n",
      "reward: -1\n",
      "done: False\n",
      "info: {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(2)\n",
    "print('state: {}'.format(state))\n",
    "print('reward: {}'.format(reward))\n",
    "print('done: {}'.format(done))\n",
    "print('info: {}'.format(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f70ce673a1ce79710aa0167eb89ab823252194a5"
   },
   "source": [
    "These outputs are the 'bread and butter' of reinforcement learning\n",
    "* state- Current situation of the agent in the environment\n",
    "* reward- Feedback from an action by the agent in the environment\n",
    "* done- Boolean indicating whether the agent has terminated or completed its environment\n",
    "* info- Diagnositic information about the agents last action. \n",
    "\n",
    "\n",
    "You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "796af916ac4e94ec3aa16a54a806e43e28c92b3d"
   },
   "source": [
    "Reward is kind of easy to misinterpret, it's normally a positive, however, in reinforcement learning it can either a positive or a negative.\n",
    "\n",
    "**Positive**\n",
    "* The agent receives `+20` points for a successful drop off\n",
    "\n",
    "**Negative**\n",
    "* The agent receives `-10` points for each mistake it makes when picking up or dropping off a passenger\n",
    "* The agent receives `-1` point for every step it makes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "79f800aefad0cec783ea4a641a47b410d3bdfc40"
   },
   "source": [
    "*Every breath you take*\n",
    "\n",
    "*Every move you make*\n",
    "\n",
    "*Every bond you break*\n",
    "\n",
    "*Every step you take*\n",
    "\n",
    "*I'll be <s>watching</s> rewarding you (with -1) *\n",
    "\n",
    "The reason why we give a negative reward at each turn, it basically forces the agent to find the quickest possible solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e4f7fdb27f92a7895cc570c713312919a8f40bb"
   },
   "source": [
    "Now what?\n",
    "\n",
    "Well we have the agent, environment and rewards. Lets explore.\n",
    "\n",
    "When we build any machine learning model we want to start simple. \n",
    "\n",
    "We know our objective is to pick up and drive our passenger to their destination, let's pick random actions until we get it right.\n",
    "\n",
    "Use ` env.action_space.sample()` to take random actions until you have a successful trip.\n",
    "\n",
    "**Tip** Think how a successful trip is rewarded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "045508900840032373e5f67c0b831cc2daf87bfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random driving took 3768 steps to complete a journey\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "reward = None\n",
    "steps = 0\n",
    "\n",
    "while reward != 20:\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    steps += 1\n",
    "\n",
    "print('Random driving took {} steps to complete a journey'.format(steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d23c7b6610873f4afacaa5ecd6d7948ad1e69008"
   },
   "source": [
    "Run it a few more times and see how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "851d7a7c93deea6dc4c53a0e9e0303385b74bb7a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of steps for a random drive 2177.6666666666665\n"
     ]
    }
   ],
   "source": [
    "random_driving_store = []\n",
    "for i in range(1,100):\n",
    "    state = env.reset()\n",
    "    reward = None\n",
    "    steps = 0\n",
    "\n",
    "    while reward != 20:\n",
    "        state, reward, done, info = env.step(env.action_space.sample())\n",
    "        steps += 1\n",
    "\n",
    "    random_driving_store.append(steps)\n",
    "    \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure()\n",
    "plt.hlines(0.5,0.5,2)  # Draw a horizontal line\n",
    "plt.eventplot(random_driving_store, orientation='horizontal', colors='k')\n",
    "\n",
    "plt.show()\n",
    "print('Average number of steps for a random drive {}'.format(np.mean(random_driving_store)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "278e2352413ce71a913024099346a3180b9aaf69"
   },
   "source": [
    "No surprises here, asigning a random action will get our taxi nowhere fast.\n",
    "\n",
    "We need a way for our agent to learn how to manouver the mean streets of the gym. Lets try building a `Q-table`.\n",
    "\n",
    "Think of a Q-table initially as a blank map of the environment. Your agent will navigate the space and updates the table with anything it finds interesting.\n",
    "\n",
    "With each iteration the agent gets more and more information about the environment. In no time at all, your agent is whizzing around the environment.\n",
    "\n",
    "Your map has now become a cheatsheet!\n",
    "\n",
    "The Q-table is made up of rows equal to the number of `states` and columns equal to the number of `actions`.\n",
    "\n",
    "The value at cell indicates the expected reward for an action given that state.\n",
    "\n",
    "Let's build our Q-table..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "02602be8087ca7f4c5d1474740a9bd578d04395e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of possible states: 500\n",
      "number of possible actions: 6\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print('number of possible states: {}'.format(state_size))\n",
    "print('number of possible actions: {}'.format(action_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf0a6673c1311bb359c1228e8b13a6bf26d39503"
   },
   "source": [
    "We've now got the dimensions of our Q-table, let's initialize our Q-table as a blank canvas (all zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "2041499ca99c3bc369aca2f61c44b0f1ad2e1523"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros([state_size, action_size])\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ed790d6fa53f1ec9102e542915e62b096e27aa1"
   },
   "source": [
    "Now we have the empty table, we get our agent to update it with information.\n",
    "\n",
    "The Q-learning algorithm Process:\n",
    "![process](https://cdn-images-1.medium.com/max/1600/1*QeoQEqWYYPs1P8yUwyaJVQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca9b3e9f8ab0955278f8ad9fddd61a5bc0b94341"
   },
   "source": [
    "1. Initialize the Q table \n",
    " * Done\n",
    "2. Choose an action\n",
    " * Pick an action `a` in the agents current state `s` based selection criteria of Q-value estimates\n",
    "3. Perform action\n",
    " * Perform the action chosen\n",
    "4. Measure reward\n",
    " * Given the action taken `a` what is the new observed outcome state `s’` and reward `r`\n",
    " 5. Update Q \n",
    "  * Use Bellman equation to update Q-table... \n",
    " \n",
    "![bellman](https://cdn-images-1.medium.com/max/2600/1*jmcVWHHbzCxDc-irBy9JTw.png) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ummmmm sorry, what?\n",
    "\n",
    "The Bellman equation is an optimization technique used in dynamic programming. It aims to find the greatest value between a decision at the current state and a new state. By breaking the dynamic optimization problem into a set of smaller problems it makes the overarchng problem easier to solve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "71e7b5418b2b8b3a1b946592a774761b5c4eb63a"
   },
   "source": [
    "We will now code up this Q-learning process to build our optimized Q-table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "78cc70c90a58cc830ea0f8662c3943f530f9cfbb"
   },
   "outputs": [],
   "source": [
    "total_reward = 0\n",
    "learning_rate = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "487df6d33bafd431419231e32a3cecd14124e9b6"
   },
   "source": [
    "We will start with a single episode, ie the episode completes when the agent fails or completes the trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "552e6cecb3b31c0a09653a1e4877e1a920460003",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward for this episode: -542\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "total_reward, reward = 0,0\n",
    "state = env.reset()\n",
    "while done != True: # Keeps making actions until episode completes\n",
    "        action = np.argmax(Q[state]) # Finds the action with the greatest reward. TIP each state is a row in the Q-table, find the best action at this state by finding the max value\n",
    "        new_state, reward, done, info = env.step(action) #Takes the action with the greatest reward\n",
    "        Q[state, action] += learning_rate * (reward + np.max(Q[new_state]) - Q[state, action]) # Updates our Q-table based on the state and actions. TIP If your stuck have a look at this pseudo code below\n",
    "        total_reward += reward # Update our total reward\n",
    "        state = new_state # Update our current state\n",
    "#         env.render() # Print the current agent-environment interaction\n",
    "print('Total reward for this episode: {}'.format(total_reward))\n",
    "\n",
    "\n",
    "# New Q value = Current Q value + learning rate * (Reward + (maximum value of new state) — Current Q value )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aa42c540e2a33248e262bc139f3d1ad4b589929f"
   },
   "source": [
    "Now we've built the logic, it's time to learn and build our Q-table across episodes. This will be easy, we'll stick our previous code in a `for loop` and iterate through `2000 episodes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "1cf3aff1f99c6478530c262a200d1e01acd4f278"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 Total Reward: -160\n",
      "Episode 100 Total Reward: -98\n",
      "Episode 150 Total Reward: -171\n",
      "Episode 200 Total Reward: 7\n",
      "Episode 250 Total Reward: -13\n",
      "Episode 300 Total Reward: -43\n",
      "Episode 350 Total Reward: 7\n",
      "Episode 400 Total Reward: -5\n",
      "Episode 450 Total Reward: 5\n",
      "Episode 500 Total Reward: 7\n",
      "Episode 550 Total Reward: 11\n",
      "Episode 600 Total Reward: 11\n",
      "Episode 650 Total Reward: 12\n",
      "Episode 700 Total Reward: 10\n",
      "Episode 750 Total Reward: 12\n",
      "Episode 800 Total Reward: 11\n",
      "Episode 850 Total Reward: 11\n",
      "Episode 900 Total Reward: -14\n",
      "Episode 950 Total Reward: 11\n",
      "Episode 1000 Total Reward: 10\n",
      "Episode 1050 Total Reward: 3\n",
      "Episode 1100 Total Reward: 0\n",
      "Episode 1150 Total Reward: 6\n",
      "Episode 1200 Total Reward: 10\n",
      "Episode 1250 Total Reward: 7\n",
      "Episode 1300 Total Reward: 9\n",
      "Episode 1350 Total Reward: 7\n",
      "Episode 1400 Total Reward: 11\n",
      "Episode 1450 Total Reward: 12\n",
      "Episode 1500 Total Reward: 12\n",
      "Episode 1550 Total Reward: 9\n",
      "Episode 1600 Total Reward: 9\n",
      "Episode 1650 Total Reward: 13\n",
      "Episode 1700 Total Reward: 8\n",
      "Episode 1750 Total Reward: 10\n",
      "Episode 1800 Total Reward: 7\n",
      "Episode 1850 Total Reward: 10\n",
      "Episode 1900 Total Reward: 6\n",
      "Episode 1950 Total Reward: 10\n",
      "Episode 2000 Total Reward: 8\n"
     ]
    }
   ],
   "source": [
    "Q = np.zeros([state_size, action_size])\n",
    "\n",
    "total_reward = 0\n",
    "learning_rate = 0.7\n",
    "\n",
    "for episode in range(1,2001):\n",
    "    done = False\n",
    "    total_reward, reward = 0,0\n",
    "    state = env.reset()\n",
    "    while done != True:\n",
    "        action = np.argmax(Q[state])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        Q[state, action] += learning_rate * (reward + np.max(Q[new_state]) - Q[state, action])\n",
    "        total_reward += reward\n",
    "        state = new_state   \n",
    "    if episode % 50 == 0:\n",
    "        print('Episode {} Total Reward: {}'.format(episode, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ca9c2e6c598cc9a989fd4b8e07dee1501d281df"
   },
   "source": [
    "# Multi armed bandits\n",
    "\n",
    "In the last exercise we were playing with taxis in a highly sterile environment. What we're going to do now is apply a reinforcement learning technique known as a multi armed bandit in a business case scenario.\n",
    "\n",
    "The name multi armed bandit comes from a type of slot machine. You pull the lever, bleep bloop and either the machine pays out some money, or it doesn't.\n",
    "\n",
    "You're faced with 6 slot machines with different probabilities of a payout. If you knew which one had the highest likelihood of a payout you would just stick with that one, right? \n",
    "The aim of a multi armed bandit is find the action with the greatest amount of reward, while still earning reward during this exploration phase.\n",
    "\n",
    "This method of determining probability distrobutions while exploiting the most successful can be used in a huge variety of industries.\n",
    "* **Clinical trials**- Exploration is akin to identifying an optimum treatment, while the exploitation occurs when treating patients efficiently as rapidly as possible from when the trial begins.\n",
    "* **Game design**- Creating and experimenting with variants of a game mechanic, more players will be allocated to the variant with the greater level of success, thus ensuring maximized user interaction throughout the experiment.\n",
    "\n",
    "However, the most used application of a multi armed bandit is as a turbo charged A/B test for product and advertising.\n",
    "\n",
    "A/B testing is by far the most common way to gage user preference. The main problem of traditional A/B tests is that it is split into two separate stages exploration and **then** exploitation. \n",
    "\n",
    "Wheras multi armed bandits combine both these phases, ie gives you the ability to earn a reward whilst still trying to find the optimum action.\n",
    "\n",
    "The goal of this next exercise to optimize the performance of your companies display ads. You make money everytime a user interacts with the ad. Your collegue has whipped up 10 variations of the ad, let's see if we can find the highest performing variant!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9125e40cc63e98447f5ce0155324c18b22b79b6"
   },
   "source": [
    "![Multi-armed-bandit](https://conversionxl.com/wp-content/uploads/2015/09/multiarmedbandit.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "53d8b6d6d33aae268a4c0b716c76ea14987c06c1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "653295400d22a2cbc8c0b270c507470005e9c89f"
   },
   "source": [
    "We're going to have to do a bit of pretending here, the data we will be working with is a static file (not streamed data). Each row represents a user and each column represents the advert that user had seen. If the user clicked the ad, the cell has a value of 1, else it's 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "5c62d0debb6b3961453313908239fa0bcc53d0c6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First user clicked Ad 1, 5 and 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ad 1</th>\n",
       "      <th>Ad 2</th>\n",
       "      <th>Ad 3</th>\n",
       "      <th>Ad 4</th>\n",
       "      <th>Ad 5</th>\n",
       "      <th>Ad 6</th>\n",
       "      <th>Ad 7</th>\n",
       "      <th>Ad 8</th>\n",
       "      <th>Ad 9</th>\n",
       "      <th>Ad 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ad 1  Ad 2  Ad 3  Ad 4  Ad 5  Ad 6  Ad 7  Ad 8  Ad 9  Ad 10\n",
       "0     1     0     0     0     1     0     0     0     1      0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset ../input/ad-datacsv/Ads_Optimisation.csv\n",
    "dataset = pd.read_csv('../input/ad-datacsv/Ads_Optimisation.csv')\n",
    "print('First user clicked Ad 1, 5 and 9')\n",
    "dataset.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba97a6f22ffe73707676b7e138a13774f2fadcbe"
   },
   "source": [
    "Just like the last exercise it's best practice to start benchmarking with an agent taking random actions.\n",
    "\n",
    "Let's start with assigning users to a random advert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "9748e56a9774287ba8c23e220dc6a1b00c9c7a17"
   },
   "outputs": [],
   "source": [
    "number_of_users = 10000\n",
    "number_of_ads = 10\n",
    "ads_selected = []\n",
    "total_reward = 0\n",
    "for user in range(0, number_of_users):\n",
    "    ad_picked = random.randrange(number_of_ads)\n",
    "    ads_selected.append(ad_picked)\n",
    "    reward = dataset.values[user, ad_picked]\n",
    "    total_reward = total_reward + reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7e2c844a612d3c3d4036c4c9851d44f7f3c9da5"
   },
   "source": [
    "The results are in from the random assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "6226b8c98f9a5633e0586746bc53a05858641e47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1302"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "2e2bb7f499559e61ce3b69ade8b15f7e9b4c0594"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    0.1043\n",
       "0    0.1028\n",
       "3    0.1024\n",
       "2    0.1001\n",
       "6    0.0991\n",
       "5    0.0988\n",
       "4    0.0987\n",
       "8    0.0987\n",
       "9    0.0984\n",
       "1    0.0967\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(ads_selected).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "deb4cdc693ebcafa1342605acbe76098420cc1d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f54f8eaf208>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(ads_selected).value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b6e13e2b5f61b30b6388bdc12656dd09950f89b"
   },
   "source": [
    "A good multi arm bandit with clever user assignment should give us a clear winner. As you can see from our results, there is no clear winner.\n",
    "\n",
    "Total reward for the random selection algorithm comes out to be ~1000. \n",
    "\n",
    "As this algorithm is not learning anything, it has no intuition into which ad is giving the maximum return. Therefore we would expect out of 10000 users it only gets ~1000, basically, a 1/10 chance, which makes sense as there are only 10 ads to choose from.\n",
    "\n",
    "Random!\n",
    "\n",
    "There must be a better way to optimize this problem...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f59a7039cfb95d97265c4408786c875c5d9a0a0c"
   },
   "source": [
    "Please enter Upper Confidence Bound (UCB)....\n",
    "\n",
    "This is one of the most commonly used optimization processes for multi armed bandits. The main point to grasp about this algorithm is that the less familiar the bandit is with a particular action, the more curious it becomes, and in turn increases the chance to explore said action.\n",
    "\n",
    "Check out the graph below.\n",
    "![dist](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/09/im_18.jpg)\n",
    "\n",
    "\n",
    "We can see the distribution of three different adverts above. You notice that a1 is much more spread out that either a2 or a3. This high level of spread indicates that this advert has not been explored enough. Next iteration the bandit will make a1 its top priority for exploration. It will keep doing this until the spread of that adverts reward meets a desired threshold.\n",
    "\n",
    "\n",
    "Here's a bit of pseudo code to help you understand the algorithm a bit more clearly\n",
    "\n",
    "Step 1. Run each of the adverts once, this will give us an ititial idea of the reward landscape\n",
    "Step 2. For each iteration of t\n",
    "Step 3. Count number of times each ad was shown so far (Nt(a))\n",
    "Step 4. Plug values into the following expression\n",
    "![formula](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/09/im_19.jpg)\n",
    "\n",
    "Step 5. Calculate and store the reward\n",
    "\n",
    "If this seems a bit too complicated don't worry, it was for me. I've grabbed some code that will help us. Try and follow along..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "23c6e8af7710a182c5b42cc7d4d50ecdd7db0eea"
   },
   "outputs": [],
   "source": [
    "def UCB_multi_armed_bandit(number_of_users, number_of_ads, dataset):\n",
    "\n",
    "    ads_selected = []\n",
    "    numbers_of_selections = [0] * number_of_ads\n",
    "    sums_of_reward = [0] * number_of_ads\n",
    "    total_reward = 0\n",
    "\n",
    "    for user in range(0, number_of_users):\n",
    "        ad = 0\n",
    "        max_upper_bound = 0\n",
    "        for i in range(0, number_of_ads):\n",
    "            if (numbers_of_selections[i] > 0):\n",
    "                average_reward = sums_of_reward[i] / numbers_of_selections[i]\n",
    "                delta_i = math.sqrt(2 * math.log(user+1) / numbers_of_selections[i])\n",
    "                upper_bound = average_reward + delta_i\n",
    "            else:\n",
    "                upper_bound = 1e400\n",
    "            if upper_bound > max_upper_bound:\n",
    "                max_upper_bound = upper_bound\n",
    "                ad = i\n",
    "        ads_selected.append(ad)\n",
    "        numbers_of_selections[ad] += 1\n",
    "        reward = dataset.values[user, ad]\n",
    "        sums_of_reward[ad] += reward\n",
    "        total_reward += reward\n",
    "    return ads_selected, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "6c4bbb7d960ab595dff257e74628b4bdf66f0177"
   },
   "outputs": [],
   "source": [
    "ads_selected, total_rewards = UCB_multi_armed_bandit(10000, 10, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "bd553fb57337904aa931b5ab629cb9db90d3fb6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f54fca9fa90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(ads_selected).head(1500).value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "38944cf0a7743d039b981b5de3484a52e941613a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2125"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fcc316e48b600c2f7fa8c64dd75eaf1397b8cb72"
   },
   "source": [
    "Check that out! The total reward is much greater now and from the graph we have an ad that is clearly outperforming the rest, ad 5! (remember zero indexing is a thing in python). \n",
    "\n",
    "Since we had the the entire dataset from the start, we can check to see which ad did in fact have the highest click rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "2231b4138f5f77c1a2345a7fa5436163c58abd54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ad 1     0.1703\n",
       "Ad 2     0.1295\n",
       "Ad 3     0.0728\n",
       "Ad 4     0.1196\n",
       "Ad 5     0.2695\n",
       "Ad 6     0.0126\n",
       "Ad 7     0.1112\n",
       "Ad 8     0.2091\n",
       "Ad 9     0.0952\n",
       "Ad 10    0.0489\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sum()/len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "94718358e51908bb4cacf5fbc4c667fb324e15eb"
   },
   "source": [
    "There we go! Our bandit was correct. Ad 5 was the top performer.\n",
    "\n",
    "That's the main content complete for this months meetup. If you have time why don't you check out the extra exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f9f1924160a6c9489287ea1f92165d68b10264fb"
   },
   "source": [
    "# Extra content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "05aec415731b91c7236449a1fc3a4fca3efaa4c2"
   },
   "source": [
    "# Greedy epsilon algorithm\n",
    "Here's another algorithm for learning `greedy epsilon`. Have a go with this code and try tweaking the hyperpara****meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "45e29b9a9859a61cc3cc634499bed962092e2c93"
   },
   "outputs": [],
   "source": [
    "total_episodes = 20000        # Total episodes\n",
    "total_test_episodes = 100     # Total test episodes\n",
    "max_steps = 99                # Max steps per episode\n",
    "\n",
    "learning_rate = 0.7           # Learning rate\n",
    "gamma = 0.618                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.01             # Exponential decay rate for exploration prob\n",
    "\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "36ff75b13a91d803d70cadf3bae81d4b1567a01a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 5.41525\n",
      "[[  0.           0.           0.           0.           0.\n",
      "    0.        ]\n",
      " [ -2.03101391  -1.47220892  -1.93861517  -1.44879596  -0.72512948\n",
      "  -10.50307739]\n",
      " [ -2.21044815   0.43833691  -0.73701282   0.37239713   2.33782249\n",
      "   -8.57973158]\n",
      " ...\n",
      " [ -1.94631341   5.39966688  -1.94631341  -1.87956996  -9.1\n",
      "   -7.        ]\n",
      " [ -2.41325649  -1.45834677  -2.35377987  -2.3753849  -10.95675082\n",
      "  -10.79630496]\n",
      " [ 18.21933513   9.64128245  17.83968317  31.35602094   9.37443907\n",
      "    8.87911438]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "b586b618d3ca5ed7eeae6690ee370201663d6881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "EPISODE  0\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Number of steps 12\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Number of steps 13\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Number of steps 8\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Number of steps 12\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Number of steps 15\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            env.render()\n",
    "            \n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9b43f863a7db7794b1ebe731dfdcf780b81084ec"
   },
   "source": [
    "# Start exploring the frozen lake environment\n",
    "OpenAI give us pleanty more environments to choose from. Check out the Frozen lake environment here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "78cd3d15a4a478af8e54242aa9a86a3e937127e8"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "85f47b6687a97f8dabfd63ea6cbb75c39a73d3a8"
   },
   "source": [
    "# Or check out any of these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "ff43c97e3996bf532d73bea0142aa003de213fb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([EnvSpec(Copy-v0), EnvSpec(RepeatCopy-v0), EnvSpec(ReversedAddition-v0), EnvSpec(ReversedAddition3-v0), EnvSpec(DuplicatedInput-v0), EnvSpec(Reverse-v0), EnvSpec(CartPole-v0), EnvSpec(CartPole-v1), EnvSpec(MountainCar-v0), EnvSpec(MountainCarContinuous-v0), EnvSpec(Pendulum-v0), EnvSpec(Acrobot-v1), EnvSpec(LunarLander-v2), EnvSpec(LunarLanderContinuous-v2), EnvSpec(BipedalWalker-v2), EnvSpec(BipedalWalkerHardcore-v2), EnvSpec(CarRacing-v0), EnvSpec(Blackjack-v0), EnvSpec(KellyCoinflip-v0), EnvSpec(KellyCoinflipGeneralized-v0), EnvSpec(FrozenLake-v0), EnvSpec(FrozenLake8x8-v0), EnvSpec(CliffWalking-v0), EnvSpec(NChain-v0), EnvSpec(Roulette-v0), EnvSpec(Taxi-v2), EnvSpec(GuessingGame-v0), EnvSpec(HotterColder-v0), EnvSpec(Reacher-v2), EnvSpec(Pusher-v2), EnvSpec(Thrower-v2), EnvSpec(Striker-v2), EnvSpec(InvertedPendulum-v2), EnvSpec(InvertedDoublePendulum-v2), EnvSpec(HalfCheetah-v2), EnvSpec(Hopper-v2), EnvSpec(Swimmer-v2), EnvSpec(Walker2d-v2), EnvSpec(Ant-v2), EnvSpec(Humanoid-v2), EnvSpec(HumanoidStandup-v2), EnvSpec(FetchSlide-v1), EnvSpec(FetchPickAndPlace-v1), EnvSpec(FetchReach-v1), EnvSpec(FetchPush-v1), EnvSpec(HandReach-v0), EnvSpec(HandManipulateBlockRotateZ-v0), EnvSpec(HandManipulateBlockRotateParallel-v0), EnvSpec(HandManipulateBlockRotateXYZ-v0), EnvSpec(HandManipulateBlockFull-v0), EnvSpec(HandManipulateBlock-v0), EnvSpec(HandManipulateEggRotate-v0), EnvSpec(HandManipulateEggFull-v0), EnvSpec(HandManipulateEgg-v0), EnvSpec(HandManipulatePenRotate-v0), EnvSpec(HandManipulatePenFull-v0), EnvSpec(HandManipulatePen-v0), EnvSpec(FetchSlideDense-v1), EnvSpec(FetchPickAndPlaceDense-v1), EnvSpec(FetchReachDense-v1), EnvSpec(FetchPushDense-v1), EnvSpec(HandReachDense-v0), EnvSpec(HandManipulateBlockRotateZDense-v0), EnvSpec(HandManipulateBlockRotateParallelDense-v0), EnvSpec(HandManipulateBlockRotateXYZDense-v0), EnvSpec(HandManipulateBlockFullDense-v0), EnvSpec(HandManipulateBlockDense-v0), EnvSpec(HandManipulateEggRotateDense-v0), EnvSpec(HandManipulateEggFullDense-v0), EnvSpec(HandManipulateEggDense-v0), EnvSpec(HandManipulatePenRotateDense-v0), EnvSpec(HandManipulatePenFullDense-v0), EnvSpec(HandManipulatePenDense-v0), EnvSpec(AirRaid-v0), EnvSpec(AirRaid-v4), EnvSpec(AirRaidDeterministic-v0), EnvSpec(AirRaidDeterministic-v4), EnvSpec(AirRaidNoFrameskip-v0), EnvSpec(AirRaidNoFrameskip-v4), EnvSpec(AirRaid-ram-v0), EnvSpec(AirRaid-ram-v4), EnvSpec(AirRaid-ramDeterministic-v0), EnvSpec(AirRaid-ramDeterministic-v4), EnvSpec(AirRaid-ramNoFrameskip-v0), EnvSpec(AirRaid-ramNoFrameskip-v4), EnvSpec(Alien-v0), EnvSpec(Alien-v4), EnvSpec(AlienDeterministic-v0), EnvSpec(AlienDeterministic-v4), EnvSpec(AlienNoFrameskip-v0), EnvSpec(AlienNoFrameskip-v4), EnvSpec(Alien-ram-v0), EnvSpec(Alien-ram-v4), EnvSpec(Alien-ramDeterministic-v0), EnvSpec(Alien-ramDeterministic-v4), EnvSpec(Alien-ramNoFrameskip-v0), EnvSpec(Alien-ramNoFrameskip-v4), EnvSpec(Amidar-v0), EnvSpec(Amidar-v4), EnvSpec(AmidarDeterministic-v0), EnvSpec(AmidarDeterministic-v4), EnvSpec(AmidarNoFrameskip-v0), EnvSpec(AmidarNoFrameskip-v4), EnvSpec(Amidar-ram-v0), EnvSpec(Amidar-ram-v4), EnvSpec(Amidar-ramDeterministic-v0), EnvSpec(Amidar-ramDeterministic-v4), EnvSpec(Amidar-ramNoFrameskip-v0), EnvSpec(Amidar-ramNoFrameskip-v4), EnvSpec(Assault-v0), EnvSpec(Assault-v4), EnvSpec(AssaultDeterministic-v0), EnvSpec(AssaultDeterministic-v4), EnvSpec(AssaultNoFrameskip-v0), EnvSpec(AssaultNoFrameskip-v4), EnvSpec(Assault-ram-v0), EnvSpec(Assault-ram-v4), EnvSpec(Assault-ramDeterministic-v0), EnvSpec(Assault-ramDeterministic-v4), EnvSpec(Assault-ramNoFrameskip-v0), EnvSpec(Assault-ramNoFrameskip-v4), EnvSpec(Asterix-v0), EnvSpec(Asterix-v4), EnvSpec(AsterixDeterministic-v0), EnvSpec(AsterixDeterministic-v4), EnvSpec(AsterixNoFrameskip-v0), EnvSpec(AsterixNoFrameskip-v4), EnvSpec(Asterix-ram-v0), EnvSpec(Asterix-ram-v4), EnvSpec(Asterix-ramDeterministic-v0), EnvSpec(Asterix-ramDeterministic-v4), EnvSpec(Asterix-ramNoFrameskip-v0), EnvSpec(Asterix-ramNoFrameskip-v4), EnvSpec(Asteroids-v0), EnvSpec(Asteroids-v4), EnvSpec(AsteroidsDeterministic-v0), EnvSpec(AsteroidsDeterministic-v4), EnvSpec(AsteroidsNoFrameskip-v0), EnvSpec(AsteroidsNoFrameskip-v4), EnvSpec(Asteroids-ram-v0), EnvSpec(Asteroids-ram-v4), EnvSpec(Asteroids-ramDeterministic-v0), EnvSpec(Asteroids-ramDeterministic-v4), EnvSpec(Asteroids-ramNoFrameskip-v0), EnvSpec(Asteroids-ramNoFrameskip-v4), EnvSpec(Atlantis-v0), EnvSpec(Atlantis-v4), EnvSpec(AtlantisDeterministic-v0), EnvSpec(AtlantisDeterministic-v4), EnvSpec(AtlantisNoFrameskip-v0), EnvSpec(AtlantisNoFrameskip-v4), EnvSpec(Atlantis-ram-v0), EnvSpec(Atlantis-ram-v4), EnvSpec(Atlantis-ramDeterministic-v0), EnvSpec(Atlantis-ramDeterministic-v4), EnvSpec(Atlantis-ramNoFrameskip-v0), EnvSpec(Atlantis-ramNoFrameskip-v4), EnvSpec(BankHeist-v0), EnvSpec(BankHeist-v4), EnvSpec(BankHeistDeterministic-v0), EnvSpec(BankHeistDeterministic-v4), EnvSpec(BankHeistNoFrameskip-v0), EnvSpec(BankHeistNoFrameskip-v4), EnvSpec(BankHeist-ram-v0), EnvSpec(BankHeist-ram-v4), EnvSpec(BankHeist-ramDeterministic-v0), EnvSpec(BankHeist-ramDeterministic-v4), EnvSpec(BankHeist-ramNoFrameskip-v0), EnvSpec(BankHeist-ramNoFrameskip-v4), EnvSpec(BattleZone-v0), EnvSpec(BattleZone-v4), EnvSpec(BattleZoneDeterministic-v0), EnvSpec(BattleZoneDeterministic-v4), EnvSpec(BattleZoneNoFrameskip-v0), EnvSpec(BattleZoneNoFrameskip-v4), EnvSpec(BattleZone-ram-v0), EnvSpec(BattleZone-ram-v4), EnvSpec(BattleZone-ramDeterministic-v0), EnvSpec(BattleZone-ramDeterministic-v4), EnvSpec(BattleZone-ramNoFrameskip-v0), EnvSpec(BattleZone-ramNoFrameskip-v4), EnvSpec(BeamRider-v0), EnvSpec(BeamRider-v4), EnvSpec(BeamRiderDeterministic-v0), EnvSpec(BeamRiderDeterministic-v4), EnvSpec(BeamRiderNoFrameskip-v0), EnvSpec(BeamRiderNoFrameskip-v4), EnvSpec(BeamRider-ram-v0), EnvSpec(BeamRider-ram-v4), EnvSpec(BeamRider-ramDeterministic-v0), EnvSpec(BeamRider-ramDeterministic-v4), EnvSpec(BeamRider-ramNoFrameskip-v0), EnvSpec(BeamRider-ramNoFrameskip-v4), EnvSpec(Berzerk-v0), EnvSpec(Berzerk-v4), EnvSpec(BerzerkDeterministic-v0), EnvSpec(BerzerkDeterministic-v4), EnvSpec(BerzerkNoFrameskip-v0), EnvSpec(BerzerkNoFrameskip-v4), EnvSpec(Berzerk-ram-v0), EnvSpec(Berzerk-ram-v4), EnvSpec(Berzerk-ramDeterministic-v0), EnvSpec(Berzerk-ramDeterministic-v4), EnvSpec(Berzerk-ramNoFrameskip-v0), EnvSpec(Berzerk-ramNoFrameskip-v4), EnvSpec(Bowling-v0), EnvSpec(Bowling-v4), EnvSpec(BowlingDeterministic-v0), EnvSpec(BowlingDeterministic-v4), EnvSpec(BowlingNoFrameskip-v0), EnvSpec(BowlingNoFrameskip-v4), EnvSpec(Bowling-ram-v0), EnvSpec(Bowling-ram-v4), EnvSpec(Bowling-ramDeterministic-v0), EnvSpec(Bowling-ramDeterministic-v4), EnvSpec(Bowling-ramNoFrameskip-v0), EnvSpec(Bowling-ramNoFrameskip-v4), EnvSpec(Boxing-v0), EnvSpec(Boxing-v4), EnvSpec(BoxingDeterministic-v0), EnvSpec(BoxingDeterministic-v4), EnvSpec(BoxingNoFrameskip-v0), EnvSpec(BoxingNoFrameskip-v4), EnvSpec(Boxing-ram-v0), EnvSpec(Boxing-ram-v4), EnvSpec(Boxing-ramDeterministic-v0), EnvSpec(Boxing-ramDeterministic-v4), EnvSpec(Boxing-ramNoFrameskip-v0), EnvSpec(Boxing-ramNoFrameskip-v4), EnvSpec(Breakout-v0), EnvSpec(Breakout-v4), EnvSpec(BreakoutDeterministic-v0), EnvSpec(BreakoutDeterministic-v4), EnvSpec(BreakoutNoFrameskip-v0), EnvSpec(BreakoutNoFrameskip-v4), EnvSpec(Breakout-ram-v0), EnvSpec(Breakout-ram-v4), EnvSpec(Breakout-ramDeterministic-v0), EnvSpec(Breakout-ramDeterministic-v4), EnvSpec(Breakout-ramNoFrameskip-v0), EnvSpec(Breakout-ramNoFrameskip-v4), EnvSpec(Carnival-v0), EnvSpec(Carnival-v4), EnvSpec(CarnivalDeterministic-v0), EnvSpec(CarnivalDeterministic-v4), EnvSpec(CarnivalNoFrameskip-v0), EnvSpec(CarnivalNoFrameskip-v4), EnvSpec(Carnival-ram-v0), EnvSpec(Carnival-ram-v4), EnvSpec(Carnival-ramDeterministic-v0), EnvSpec(Carnival-ramDeterministic-v4), EnvSpec(Carnival-ramNoFrameskip-v0), EnvSpec(Carnival-ramNoFrameskip-v4), EnvSpec(Centipede-v0), EnvSpec(Centipede-v4), EnvSpec(CentipedeDeterministic-v0), EnvSpec(CentipedeDeterministic-v4), EnvSpec(CentipedeNoFrameskip-v0), EnvSpec(CentipedeNoFrameskip-v4), EnvSpec(Centipede-ram-v0), EnvSpec(Centipede-ram-v4), EnvSpec(Centipede-ramDeterministic-v0), EnvSpec(Centipede-ramDeterministic-v4), EnvSpec(Centipede-ramNoFrameskip-v0), EnvSpec(Centipede-ramNoFrameskip-v4), EnvSpec(ChopperCommand-v0), EnvSpec(ChopperCommand-v4), EnvSpec(ChopperCommandDeterministic-v0), EnvSpec(ChopperCommandDeterministic-v4), EnvSpec(ChopperCommandNoFrameskip-v0), EnvSpec(ChopperCommandNoFrameskip-v4), EnvSpec(ChopperCommand-ram-v0), EnvSpec(ChopperCommand-ram-v4), EnvSpec(ChopperCommand-ramDeterministic-v0), EnvSpec(ChopperCommand-ramDeterministic-v4), EnvSpec(ChopperCommand-ramNoFrameskip-v0), EnvSpec(ChopperCommand-ramNoFrameskip-v4), EnvSpec(CrazyClimber-v0), EnvSpec(CrazyClimber-v4), EnvSpec(CrazyClimberDeterministic-v0), EnvSpec(CrazyClimberDeterministic-v4), EnvSpec(CrazyClimberNoFrameskip-v0), EnvSpec(CrazyClimberNoFrameskip-v4), EnvSpec(CrazyClimber-ram-v0), EnvSpec(CrazyClimber-ram-v4), EnvSpec(CrazyClimber-ramDeterministic-v0), EnvSpec(CrazyClimber-ramDeterministic-v4), EnvSpec(CrazyClimber-ramNoFrameskip-v0), EnvSpec(CrazyClimber-ramNoFrameskip-v4), EnvSpec(DemonAttack-v0), EnvSpec(DemonAttack-v4), EnvSpec(DemonAttackDeterministic-v0), EnvSpec(DemonAttackDeterministic-v4), EnvSpec(DemonAttackNoFrameskip-v0), EnvSpec(DemonAttackNoFrameskip-v4), EnvSpec(DemonAttack-ram-v0), EnvSpec(DemonAttack-ram-v4), EnvSpec(DemonAttack-ramDeterministic-v0), EnvSpec(DemonAttack-ramDeterministic-v4), EnvSpec(DemonAttack-ramNoFrameskip-v0), EnvSpec(DemonAttack-ramNoFrameskip-v4), EnvSpec(DoubleDunk-v0), EnvSpec(DoubleDunk-v4), EnvSpec(DoubleDunkDeterministic-v0), EnvSpec(DoubleDunkDeterministic-v4), EnvSpec(DoubleDunkNoFrameskip-v0), EnvSpec(DoubleDunkNoFrameskip-v4), EnvSpec(DoubleDunk-ram-v0), EnvSpec(DoubleDunk-ram-v4), EnvSpec(DoubleDunk-ramDeterministic-v0), EnvSpec(DoubleDunk-ramDeterministic-v4), EnvSpec(DoubleDunk-ramNoFrameskip-v0), EnvSpec(DoubleDunk-ramNoFrameskip-v4), EnvSpec(ElevatorAction-v0), EnvSpec(ElevatorAction-v4), EnvSpec(ElevatorActionDeterministic-v0), EnvSpec(ElevatorActionDeterministic-v4), EnvSpec(ElevatorActionNoFrameskip-v0), EnvSpec(ElevatorActionNoFrameskip-v4), EnvSpec(ElevatorAction-ram-v0), EnvSpec(ElevatorAction-ram-v4), EnvSpec(ElevatorAction-ramDeterministic-v0), EnvSpec(ElevatorAction-ramDeterministic-v4), EnvSpec(ElevatorAction-ramNoFrameskip-v0), EnvSpec(ElevatorAction-ramNoFrameskip-v4), EnvSpec(Enduro-v0), EnvSpec(Enduro-v4), EnvSpec(EnduroDeterministic-v0), EnvSpec(EnduroDeterministic-v4), EnvSpec(EnduroNoFrameskip-v0), EnvSpec(EnduroNoFrameskip-v4), EnvSpec(Enduro-ram-v0), EnvSpec(Enduro-ram-v4), EnvSpec(Enduro-ramDeterministic-v0), EnvSpec(Enduro-ramDeterministic-v4), EnvSpec(Enduro-ramNoFrameskip-v0), EnvSpec(Enduro-ramNoFrameskip-v4), EnvSpec(FishingDerby-v0), EnvSpec(FishingDerby-v4), EnvSpec(FishingDerbyDeterministic-v0), EnvSpec(FishingDerbyDeterministic-v4), EnvSpec(FishingDerbyNoFrameskip-v0), EnvSpec(FishingDerbyNoFrameskip-v4), EnvSpec(FishingDerby-ram-v0), EnvSpec(FishingDerby-ram-v4), EnvSpec(FishingDerby-ramDeterministic-v0), EnvSpec(FishingDerby-ramDeterministic-v4), EnvSpec(FishingDerby-ramNoFrameskip-v0), EnvSpec(FishingDerby-ramNoFrameskip-v4), EnvSpec(Freeway-v0), EnvSpec(Freeway-v4), EnvSpec(FreewayDeterministic-v0), EnvSpec(FreewayDeterministic-v4), EnvSpec(FreewayNoFrameskip-v0), EnvSpec(FreewayNoFrameskip-v4), EnvSpec(Freeway-ram-v0), EnvSpec(Freeway-ram-v4), EnvSpec(Freeway-ramDeterministic-v0), EnvSpec(Freeway-ramDeterministic-v4), EnvSpec(Freeway-ramNoFrameskip-v0), EnvSpec(Freeway-ramNoFrameskip-v4), EnvSpec(Frostbite-v0), EnvSpec(Frostbite-v4), EnvSpec(FrostbiteDeterministic-v0), EnvSpec(FrostbiteDeterministic-v4), EnvSpec(FrostbiteNoFrameskip-v0), EnvSpec(FrostbiteNoFrameskip-v4), EnvSpec(Frostbite-ram-v0), EnvSpec(Frostbite-ram-v4), EnvSpec(Frostbite-ramDeterministic-v0), EnvSpec(Frostbite-ramDeterministic-v4), EnvSpec(Frostbite-ramNoFrameskip-v0), EnvSpec(Frostbite-ramNoFrameskip-v4), EnvSpec(Gopher-v0), EnvSpec(Gopher-v4), EnvSpec(GopherDeterministic-v0), EnvSpec(GopherDeterministic-v4), EnvSpec(GopherNoFrameskip-v0), EnvSpec(GopherNoFrameskip-v4), EnvSpec(Gopher-ram-v0), EnvSpec(Gopher-ram-v4), EnvSpec(Gopher-ramDeterministic-v0), EnvSpec(Gopher-ramDeterministic-v4), EnvSpec(Gopher-ramNoFrameskip-v0), EnvSpec(Gopher-ramNoFrameskip-v4), EnvSpec(Gravitar-v0), EnvSpec(Gravitar-v4), EnvSpec(GravitarDeterministic-v0), EnvSpec(GravitarDeterministic-v4), EnvSpec(GravitarNoFrameskip-v0), EnvSpec(GravitarNoFrameskip-v4), EnvSpec(Gravitar-ram-v0), EnvSpec(Gravitar-ram-v4), EnvSpec(Gravitar-ramDeterministic-v0), EnvSpec(Gravitar-ramDeterministic-v4), EnvSpec(Gravitar-ramNoFrameskip-v0), EnvSpec(Gravitar-ramNoFrameskip-v4), EnvSpec(Hero-v0), EnvSpec(Hero-v4), EnvSpec(HeroDeterministic-v0), EnvSpec(HeroDeterministic-v4), EnvSpec(HeroNoFrameskip-v0), EnvSpec(HeroNoFrameskip-v4), EnvSpec(Hero-ram-v0), EnvSpec(Hero-ram-v4), EnvSpec(Hero-ramDeterministic-v0), EnvSpec(Hero-ramDeterministic-v4), EnvSpec(Hero-ramNoFrameskip-v0), EnvSpec(Hero-ramNoFrameskip-v4), EnvSpec(IceHockey-v0), EnvSpec(IceHockey-v4), EnvSpec(IceHockeyDeterministic-v0), EnvSpec(IceHockeyDeterministic-v4), EnvSpec(IceHockeyNoFrameskip-v0), EnvSpec(IceHockeyNoFrameskip-v4), EnvSpec(IceHockey-ram-v0), EnvSpec(IceHockey-ram-v4), EnvSpec(IceHockey-ramDeterministic-v0), EnvSpec(IceHockey-ramDeterministic-v4), EnvSpec(IceHockey-ramNoFrameskip-v0), EnvSpec(IceHockey-ramNoFrameskip-v4), EnvSpec(Jamesbond-v0), EnvSpec(Jamesbond-v4), EnvSpec(JamesbondDeterministic-v0), EnvSpec(JamesbondDeterministic-v4), EnvSpec(JamesbondNoFrameskip-v0), EnvSpec(JamesbondNoFrameskip-v4), EnvSpec(Jamesbond-ram-v0), EnvSpec(Jamesbond-ram-v4), EnvSpec(Jamesbond-ramDeterministic-v0), EnvSpec(Jamesbond-ramDeterministic-v4), EnvSpec(Jamesbond-ramNoFrameskip-v0), EnvSpec(Jamesbond-ramNoFrameskip-v4), EnvSpec(JourneyEscape-v0), EnvSpec(JourneyEscape-v4), EnvSpec(JourneyEscapeDeterministic-v0), EnvSpec(JourneyEscapeDeterministic-v4), EnvSpec(JourneyEscapeNoFrameskip-v0), EnvSpec(JourneyEscapeNoFrameskip-v4), EnvSpec(JourneyEscape-ram-v0), EnvSpec(JourneyEscape-ram-v4), EnvSpec(JourneyEscape-ramDeterministic-v0), EnvSpec(JourneyEscape-ramDeterministic-v4), EnvSpec(JourneyEscape-ramNoFrameskip-v0), EnvSpec(JourneyEscape-ramNoFrameskip-v4), EnvSpec(Kangaroo-v0), EnvSpec(Kangaroo-v4), EnvSpec(KangarooDeterministic-v0), EnvSpec(KangarooDeterministic-v4), EnvSpec(KangarooNoFrameskip-v0), EnvSpec(KangarooNoFrameskip-v4), EnvSpec(Kangaroo-ram-v0), EnvSpec(Kangaroo-ram-v4), EnvSpec(Kangaroo-ramDeterministic-v0), EnvSpec(Kangaroo-ramDeterministic-v4), EnvSpec(Kangaroo-ramNoFrameskip-v0), EnvSpec(Kangaroo-ramNoFrameskip-v4), EnvSpec(Krull-v0), EnvSpec(Krull-v4), EnvSpec(KrullDeterministic-v0), EnvSpec(KrullDeterministic-v4), EnvSpec(KrullNoFrameskip-v0), EnvSpec(KrullNoFrameskip-v4), EnvSpec(Krull-ram-v0), EnvSpec(Krull-ram-v4), EnvSpec(Krull-ramDeterministic-v0), EnvSpec(Krull-ramDeterministic-v4), EnvSpec(Krull-ramNoFrameskip-v0), EnvSpec(Krull-ramNoFrameskip-v4), EnvSpec(KungFuMaster-v0), EnvSpec(KungFuMaster-v4), EnvSpec(KungFuMasterDeterministic-v0), EnvSpec(KungFuMasterDeterministic-v4), EnvSpec(KungFuMasterNoFrameskip-v0), EnvSpec(KungFuMasterNoFrameskip-v4), EnvSpec(KungFuMaster-ram-v0), EnvSpec(KungFuMaster-ram-v4), EnvSpec(KungFuMaster-ramDeterministic-v0), EnvSpec(KungFuMaster-ramDeterministic-v4), EnvSpec(KungFuMaster-ramNoFrameskip-v0), EnvSpec(KungFuMaster-ramNoFrameskip-v4), EnvSpec(MontezumaRevenge-v0), EnvSpec(MontezumaRevenge-v4), EnvSpec(MontezumaRevengeDeterministic-v0), EnvSpec(MontezumaRevengeDeterministic-v4), EnvSpec(MontezumaRevengeNoFrameskip-v0), EnvSpec(MontezumaRevengeNoFrameskip-v4), EnvSpec(MontezumaRevenge-ram-v0), EnvSpec(MontezumaRevenge-ram-v4), EnvSpec(MontezumaRevenge-ramDeterministic-v0), EnvSpec(MontezumaRevenge-ramDeterministic-v4), EnvSpec(MontezumaRevenge-ramNoFrameskip-v0), EnvSpec(MontezumaRevenge-ramNoFrameskip-v4), EnvSpec(MsPacman-v0), EnvSpec(MsPacman-v4), EnvSpec(MsPacmanDeterministic-v0), EnvSpec(MsPacmanDeterministic-v4), EnvSpec(MsPacmanNoFrameskip-v0), EnvSpec(MsPacmanNoFrameskip-v4), EnvSpec(MsPacman-ram-v0), EnvSpec(MsPacman-ram-v4), EnvSpec(MsPacman-ramDeterministic-v0), EnvSpec(MsPacman-ramDeterministic-v4), EnvSpec(MsPacman-ramNoFrameskip-v0), EnvSpec(MsPacman-ramNoFrameskip-v4), EnvSpec(NameThisGame-v0), EnvSpec(NameThisGame-v4), EnvSpec(NameThisGameDeterministic-v0), EnvSpec(NameThisGameDeterministic-v4), EnvSpec(NameThisGameNoFrameskip-v0), EnvSpec(NameThisGameNoFrameskip-v4), EnvSpec(NameThisGame-ram-v0), EnvSpec(NameThisGame-ram-v4), EnvSpec(NameThisGame-ramDeterministic-v0), EnvSpec(NameThisGame-ramDeterministic-v4), EnvSpec(NameThisGame-ramNoFrameskip-v0), EnvSpec(NameThisGame-ramNoFrameskip-v4), EnvSpec(Phoenix-v0), EnvSpec(Phoenix-v4), EnvSpec(PhoenixDeterministic-v0), EnvSpec(PhoenixDeterministic-v4), EnvSpec(PhoenixNoFrameskip-v0), EnvSpec(PhoenixNoFrameskip-v4), EnvSpec(Phoenix-ram-v0), EnvSpec(Phoenix-ram-v4), EnvSpec(Phoenix-ramDeterministic-v0), EnvSpec(Phoenix-ramDeterministic-v4), EnvSpec(Phoenix-ramNoFrameskip-v0), EnvSpec(Phoenix-ramNoFrameskip-v4), EnvSpec(Pitfall-v0), EnvSpec(Pitfall-v4), EnvSpec(PitfallDeterministic-v0), EnvSpec(PitfallDeterministic-v4), EnvSpec(PitfallNoFrameskip-v0), EnvSpec(PitfallNoFrameskip-v4), EnvSpec(Pitfall-ram-v0), EnvSpec(Pitfall-ram-v4), EnvSpec(Pitfall-ramDeterministic-v0), EnvSpec(Pitfall-ramDeterministic-v4), EnvSpec(Pitfall-ramNoFrameskip-v0), EnvSpec(Pitfall-ramNoFrameskip-v4), EnvSpec(Pong-v0), EnvSpec(Pong-v4), EnvSpec(PongDeterministic-v0), EnvSpec(PongDeterministic-v4), EnvSpec(PongNoFrameskip-v0), EnvSpec(PongNoFrameskip-v4), EnvSpec(Pong-ram-v0), EnvSpec(Pong-ram-v4), EnvSpec(Pong-ramDeterministic-v0), EnvSpec(Pong-ramDeterministic-v4), EnvSpec(Pong-ramNoFrameskip-v0), EnvSpec(Pong-ramNoFrameskip-v4), EnvSpec(Pooyan-v0), EnvSpec(Pooyan-v4), EnvSpec(PooyanDeterministic-v0), EnvSpec(PooyanDeterministic-v4), EnvSpec(PooyanNoFrameskip-v0), EnvSpec(PooyanNoFrameskip-v4), EnvSpec(Pooyan-ram-v0), EnvSpec(Pooyan-ram-v4), EnvSpec(Pooyan-ramDeterministic-v0), EnvSpec(Pooyan-ramDeterministic-v4), EnvSpec(Pooyan-ramNoFrameskip-v0), EnvSpec(Pooyan-ramNoFrameskip-v4), EnvSpec(PrivateEye-v0), EnvSpec(PrivateEye-v4), EnvSpec(PrivateEyeDeterministic-v0), EnvSpec(PrivateEyeDeterministic-v4), EnvSpec(PrivateEyeNoFrameskip-v0), EnvSpec(PrivateEyeNoFrameskip-v4), EnvSpec(PrivateEye-ram-v0), EnvSpec(PrivateEye-ram-v4), EnvSpec(PrivateEye-ramDeterministic-v0), EnvSpec(PrivateEye-ramDeterministic-v4), EnvSpec(PrivateEye-ramNoFrameskip-v0), EnvSpec(PrivateEye-ramNoFrameskip-v4), EnvSpec(Qbert-v0), EnvSpec(Qbert-v4), EnvSpec(QbertDeterministic-v0), EnvSpec(QbertDeterministic-v4), EnvSpec(QbertNoFrameskip-v0), EnvSpec(QbertNoFrameskip-v4), EnvSpec(Qbert-ram-v0), EnvSpec(Qbert-ram-v4), EnvSpec(Qbert-ramDeterministic-v0), EnvSpec(Qbert-ramDeterministic-v4), EnvSpec(Qbert-ramNoFrameskip-v0), EnvSpec(Qbert-ramNoFrameskip-v4), EnvSpec(Riverraid-v0), EnvSpec(Riverraid-v4), EnvSpec(RiverraidDeterministic-v0), EnvSpec(RiverraidDeterministic-v4), EnvSpec(RiverraidNoFrameskip-v0), EnvSpec(RiverraidNoFrameskip-v4), EnvSpec(Riverraid-ram-v0), EnvSpec(Riverraid-ram-v4), EnvSpec(Riverraid-ramDeterministic-v0), EnvSpec(Riverraid-ramDeterministic-v4), EnvSpec(Riverraid-ramNoFrameskip-v0), EnvSpec(Riverraid-ramNoFrameskip-v4), EnvSpec(RoadRunner-v0), EnvSpec(RoadRunner-v4), EnvSpec(RoadRunnerDeterministic-v0), EnvSpec(RoadRunnerDeterministic-v4), EnvSpec(RoadRunnerNoFrameskip-v0), EnvSpec(RoadRunnerNoFrameskip-v4), EnvSpec(RoadRunner-ram-v0), EnvSpec(RoadRunner-ram-v4), EnvSpec(RoadRunner-ramDeterministic-v0), EnvSpec(RoadRunner-ramDeterministic-v4), EnvSpec(RoadRunner-ramNoFrameskip-v0), EnvSpec(RoadRunner-ramNoFrameskip-v4), EnvSpec(Robotank-v0), EnvSpec(Robotank-v4), EnvSpec(RobotankDeterministic-v0), EnvSpec(RobotankDeterministic-v4), EnvSpec(RobotankNoFrameskip-v0), EnvSpec(RobotankNoFrameskip-v4), EnvSpec(Robotank-ram-v0), EnvSpec(Robotank-ram-v4), EnvSpec(Robotank-ramDeterministic-v0), EnvSpec(Robotank-ramDeterministic-v4), EnvSpec(Robotank-ramNoFrameskip-v0), EnvSpec(Robotank-ramNoFrameskip-v4), EnvSpec(Seaquest-v0), EnvSpec(Seaquest-v4), EnvSpec(SeaquestDeterministic-v0), EnvSpec(SeaquestDeterministic-v4), EnvSpec(SeaquestNoFrameskip-v0), EnvSpec(SeaquestNoFrameskip-v4), EnvSpec(Seaquest-ram-v0), EnvSpec(Seaquest-ram-v4), EnvSpec(Seaquest-ramDeterministic-v0), EnvSpec(Seaquest-ramDeterministic-v4), EnvSpec(Seaquest-ramNoFrameskip-v0), EnvSpec(Seaquest-ramNoFrameskip-v4), EnvSpec(Skiing-v0), EnvSpec(Skiing-v4), EnvSpec(SkiingDeterministic-v0), EnvSpec(SkiingDeterministic-v4), EnvSpec(SkiingNoFrameskip-v0), EnvSpec(SkiingNoFrameskip-v4), EnvSpec(Skiing-ram-v0), EnvSpec(Skiing-ram-v4), EnvSpec(Skiing-ramDeterministic-v0), EnvSpec(Skiing-ramDeterministic-v4), EnvSpec(Skiing-ramNoFrameskip-v0), EnvSpec(Skiing-ramNoFrameskip-v4), EnvSpec(Solaris-v0), EnvSpec(Solaris-v4), EnvSpec(SolarisDeterministic-v0), EnvSpec(SolarisDeterministic-v4), EnvSpec(SolarisNoFrameskip-v0), EnvSpec(SolarisNoFrameskip-v4), EnvSpec(Solaris-ram-v0), EnvSpec(Solaris-ram-v4), EnvSpec(Solaris-ramDeterministic-v0), EnvSpec(Solaris-ramDeterministic-v4), EnvSpec(Solaris-ramNoFrameskip-v0), EnvSpec(Solaris-ramNoFrameskip-v4), EnvSpec(SpaceInvaders-v0), EnvSpec(SpaceInvaders-v4), EnvSpec(SpaceInvadersDeterministic-v0), EnvSpec(SpaceInvadersDeterministic-v4), EnvSpec(SpaceInvadersNoFrameskip-v0), EnvSpec(SpaceInvadersNoFrameskip-v4), EnvSpec(SpaceInvaders-ram-v0), EnvSpec(SpaceInvaders-ram-v4), EnvSpec(SpaceInvaders-ramDeterministic-v0), EnvSpec(SpaceInvaders-ramDeterministic-v4), EnvSpec(SpaceInvaders-ramNoFrameskip-v0), EnvSpec(SpaceInvaders-ramNoFrameskip-v4), EnvSpec(StarGunner-v0), EnvSpec(StarGunner-v4), EnvSpec(StarGunnerDeterministic-v0), EnvSpec(StarGunnerDeterministic-v4), EnvSpec(StarGunnerNoFrameskip-v0), EnvSpec(StarGunnerNoFrameskip-v4), EnvSpec(StarGunner-ram-v0), EnvSpec(StarGunner-ram-v4), EnvSpec(StarGunner-ramDeterministic-v0), EnvSpec(StarGunner-ramDeterministic-v4), EnvSpec(StarGunner-ramNoFrameskip-v0), EnvSpec(StarGunner-ramNoFrameskip-v4), EnvSpec(Tennis-v0), EnvSpec(Tennis-v4), EnvSpec(TennisDeterministic-v0), EnvSpec(TennisDeterministic-v4), EnvSpec(TennisNoFrameskip-v0), EnvSpec(TennisNoFrameskip-v4), EnvSpec(Tennis-ram-v0), EnvSpec(Tennis-ram-v4), EnvSpec(Tennis-ramDeterministic-v0), EnvSpec(Tennis-ramDeterministic-v4), EnvSpec(Tennis-ramNoFrameskip-v0), EnvSpec(Tennis-ramNoFrameskip-v4), EnvSpec(TimePilot-v0), EnvSpec(TimePilot-v4), EnvSpec(TimePilotDeterministic-v0), EnvSpec(TimePilotDeterministic-v4), EnvSpec(TimePilotNoFrameskip-v0), EnvSpec(TimePilotNoFrameskip-v4), EnvSpec(TimePilot-ram-v0), EnvSpec(TimePilot-ram-v4), EnvSpec(TimePilot-ramDeterministic-v0), EnvSpec(TimePilot-ramDeterministic-v4), EnvSpec(TimePilot-ramNoFrameskip-v0), EnvSpec(TimePilot-ramNoFrameskip-v4), EnvSpec(Tutankham-v0), EnvSpec(Tutankham-v4), EnvSpec(TutankhamDeterministic-v0), EnvSpec(TutankhamDeterministic-v4), EnvSpec(TutankhamNoFrameskip-v0), EnvSpec(TutankhamNoFrameskip-v4), EnvSpec(Tutankham-ram-v0), EnvSpec(Tutankham-ram-v4), EnvSpec(Tutankham-ramDeterministic-v0), EnvSpec(Tutankham-ramDeterministic-v4), EnvSpec(Tutankham-ramNoFrameskip-v0), EnvSpec(Tutankham-ramNoFrameskip-v4), EnvSpec(UpNDown-v0), EnvSpec(UpNDown-v4), EnvSpec(UpNDownDeterministic-v0), EnvSpec(UpNDownDeterministic-v4), EnvSpec(UpNDownNoFrameskip-v0), EnvSpec(UpNDownNoFrameskip-v4), EnvSpec(UpNDown-ram-v0), EnvSpec(UpNDown-ram-v4), EnvSpec(UpNDown-ramDeterministic-v0), EnvSpec(UpNDown-ramDeterministic-v4), EnvSpec(UpNDown-ramNoFrameskip-v0), EnvSpec(UpNDown-ramNoFrameskip-v4), EnvSpec(Venture-v0), EnvSpec(Venture-v4), EnvSpec(VentureDeterministic-v0), EnvSpec(VentureDeterministic-v4), EnvSpec(VentureNoFrameskip-v0), EnvSpec(VentureNoFrameskip-v4), EnvSpec(Venture-ram-v0), EnvSpec(Venture-ram-v4), EnvSpec(Venture-ramDeterministic-v0), EnvSpec(Venture-ramDeterministic-v4), EnvSpec(Venture-ramNoFrameskip-v0), EnvSpec(Venture-ramNoFrameskip-v4), EnvSpec(VideoPinball-v0), EnvSpec(VideoPinball-v4), EnvSpec(VideoPinballDeterministic-v0), EnvSpec(VideoPinballDeterministic-v4), EnvSpec(VideoPinballNoFrameskip-v0), EnvSpec(VideoPinballNoFrameskip-v4), EnvSpec(VideoPinball-ram-v0), EnvSpec(VideoPinball-ram-v4), EnvSpec(VideoPinball-ramDeterministic-v0), EnvSpec(VideoPinball-ramDeterministic-v4), EnvSpec(VideoPinball-ramNoFrameskip-v0), EnvSpec(VideoPinball-ramNoFrameskip-v4), EnvSpec(WizardOfWor-v0), EnvSpec(WizardOfWor-v4), EnvSpec(WizardOfWorDeterministic-v0), EnvSpec(WizardOfWorDeterministic-v4), EnvSpec(WizardOfWorNoFrameskip-v0), EnvSpec(WizardOfWorNoFrameskip-v4), EnvSpec(WizardOfWor-ram-v0), EnvSpec(WizardOfWor-ram-v4), EnvSpec(WizardOfWor-ramDeterministic-v0), EnvSpec(WizardOfWor-ramDeterministic-v4), EnvSpec(WizardOfWor-ramNoFrameskip-v0), EnvSpec(WizardOfWor-ramNoFrameskip-v4), EnvSpec(YarsRevenge-v0), EnvSpec(YarsRevenge-v4), EnvSpec(YarsRevengeDeterministic-v0), EnvSpec(YarsRevengeDeterministic-v4), EnvSpec(YarsRevengeNoFrameskip-v0), EnvSpec(YarsRevengeNoFrameskip-v4), EnvSpec(YarsRevenge-ram-v0), EnvSpec(YarsRevenge-ram-v4), EnvSpec(YarsRevenge-ramDeterministic-v0), EnvSpec(YarsRevenge-ramDeterministic-v4), EnvSpec(YarsRevenge-ramNoFrameskip-v0), EnvSpec(YarsRevenge-ramNoFrameskip-v4), EnvSpec(Zaxxon-v0), EnvSpec(Zaxxon-v4), EnvSpec(ZaxxonDeterministic-v0), EnvSpec(ZaxxonDeterministic-v4), EnvSpec(ZaxxonNoFrameskip-v0), EnvSpec(ZaxxonNoFrameskip-v4), EnvSpec(Zaxxon-ram-v0), EnvSpec(Zaxxon-ram-v4), EnvSpec(Zaxxon-ramDeterministic-v0), EnvSpec(Zaxxon-ramDeterministic-v4), EnvSpec(Zaxxon-ramNoFrameskip-v0), EnvSpec(Zaxxon-ramNoFrameskip-v4), EnvSpec(CubeCrash-v0), EnvSpec(CubeCrashSparse-v0), EnvSpec(CubeCrashScreenBecomesBlack-v0), EnvSpec(MemorizeDigits-v0)])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.envs.registry.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "54f99b7facd5568f0f24b10cd1fa91eb73cc7ec1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
